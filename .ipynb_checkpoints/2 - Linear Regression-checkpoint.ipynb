{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression predicts a real-valued output based on a number of features. The learned model is a linear function that maps the one or more *explanatory* or *independent* variables to the one *dependent* variable.\n",
    "\n",
    "Input - x - Independent variables\n",
    "\n",
    "Output - y - Dependent variables\n",
    "\n",
    "Single training example - (x, y)\n",
    "\n",
    "An example dataset is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Datapoint:  [array([2, 7]) 3.2000000000000002]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the features, the independent variables. This example has two features.\n",
    "x = np.array([[2,7],\n",
    "              [1,1],\n",
    "              [3,18],\n",
    "              [2,5],\n",
    "              [1,2],\n",
    "              [3,10]])\n",
    "\n",
    "# Define the labels, the dependent variables\n",
    "y = np.array([3.2,\n",
    "              1.1,\n",
    "              6.9,\n",
    "              4.3,\n",
    "              2.1,\n",
    "              7.8])\n",
    "\n",
    "# A single data point is then:\n",
    "p1 = np.array([x[0], y[0]])\n",
    "print('Example Datapoint: ', p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, our *hypothesis* function is of the form `theta_0 + theta_1 * x`, where `theta_0` and `theta_1` are the *parameters* that define the function, whose value is learned by the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our hypothesis function estimates the dependent variable\n",
    "def hypothesis_function(x, theta_0, theta_1):\n",
    "    return theta_0 + np.matmul(x, theta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example the hypothesis function would then try and find the value of `theta_0` and `theta_1` that minimises the error in the function. An illustrative example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate:  [10  3 22  8  4 14]\n",
      "Difference:  [  6.8   1.9  15.1   3.7   1.9   6.2]\n"
     ]
    }
   ],
   "source": [
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1]) # One parameter for each feature\n",
    "\n",
    "# Our hypothesis function\n",
    "estimate_y = hypothesis_function(x, theta_0, theta_1)\n",
    "\n",
    "print('Estimate: ', estimate_y)\n",
    "print('Difference: ', np.absolute(y - estimate_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimate is clearly quite a long way off! The values from using a different set of parameters are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate:  [  5.8   0.5  15.1   4.2   1.3   8.7]\n",
      "Difference:  [ 2.6  0.6  8.2  0.1  0.8  0.9]\n"
     ]
    }
   ],
   "source": [
    "theta_0 = -0.8\n",
    "theta_1 = np.array([0.5,0.8]) # One parameter for each feature\n",
    "\n",
    "#Our hypothesis function\n",
    "estimate_y = hypothesis_function(x, theta_0, theta_1)\n",
    "\n",
    "print('Estimate: ', estimate_y)\n",
    "print('Difference: ', np.absolute(y - estimate_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "These parameter values are clearly a much better choice (whilst still not being perfect). We need a method to automatically adjust the parameter values `theta_0` and `theta_1` to minimise the difference between the estimates and the actual values of the dependent variable. We do this by defining our *loss* or *cost* function.\n",
    "\n",
    "A common loss function is the *mean squared difference*. This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(y, estimate_y):\n",
    "    return np.mean(np.square(y - estimate_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function can be used to measure how well our choice of parameters predict the dependent variable, as shown in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  55.6\n"
     ]
    }
   ],
   "source": [
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1]) # One parameter for each feature\n",
    "\n",
    "estimate_y = hypothesis_function(x, theta_0, theta_1)\n",
    "\n",
    "loss = loss_function(y, estimate_y)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12.6366666667\n"
     ]
    }
   ],
   "source": [
    "theta_0 = -0.8\n",
    "theta_1 = np.array([0.5,0.8]) # One parameter for each feature\n",
    "\n",
    "estimate_y = hypothesis_function(x, theta_0, theta_1)\n",
    "loss = loss_function(y, estimate_y)\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that the loss function is lower for the second set of parameter values, as we would expect.\n",
    "\n",
    "The next step is to adjust the parameter values in order to minimise the defined loss function. For example, adjusting the value of `theta_0` changes the loss. This can be plotted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fc2ba42f710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXh33fF5FFUBBEQMFoFdRKtahoxfZaK+oV\nXIqt2FurdWt/VW9berX9XVvB7dKKW70qbpUCLgjuVduAEkC2gCDBAGFfBUI+949zomOchIHkzJnJ\nvJ+PRx6ZOXNm5pMzJ3nn+/2e8z3m7oiIiFRUJ+4CREQkMykgREQkKQWEiIgkpYAQEZGkFBAiIpKU\nAkJERJJSQIiISFIKCBERSUoBISIiSdWLu4DqaNeunXfv3j3uMkREssrs2bPXu3v7/a2X1QHRvXt3\n8vPz4y5DRCSrmNnKVNZTF5OIiCSlgBARkaQUECIikpQCQkREklJAiIhIUgoIERFJSgEhIiJJKSBE\nRLLMX99fSeG67ZG/jwJCRCSLFK7bzm0vzmdy/qrI30sBISKSRe6dtZSG9eoy5tTDI38vBYSISJYo\nXLedKXM/47KTDqNds4aRv58CQkQkS5S3Hn6YhtYDRBgQZjbJzNaZ2fwKy39iZovMbIGZ/T5h+a1m\nVmhmi83szKjqEhHJRl+0Hganp/UA0c7m+ghwL/BY+QIzGwqMAI5x991m1iFc3he4CDgaOBR4zcyO\ndPd9EdYnIpI1JpSPPZySntYDRNiCcPe3gI0VFv8YuNPdd4frrAuXjwCecvfd7v4JUAicEFVtIiLZ\nJLH10DZNrQdI/xjEkcApZvaBmb1pZseHyzsDicdsFYXLRERy3oRZS2mU5tYDpD8g6gFtgBOBG4HJ\nZmYH8gJmNsbM8s0sv6SkJIoaRUQyRlytB0h/QBQBz3vgn0AZ0A5YDXRNWK9LuOxr3H2iu+e5e177\n9vu9Yp6ISFaLq/UA6Q+IvwFDAczsSKABsB6YAlxkZg3NrAfQC/hnmmsTEckocbYeIMKjmMzsSeA0\noJ2ZFQG3A5OASeGhr3uAUe7uwAIzmwx8DJQCY3UEk4jkugmzltK4fjytB4gwINx9ZCUPXVrJ+uOA\ncVHVIyKSTcpbD1efekQsrQfQmdQiIhmpvPXww1N6xFaDAkJEJMMUrtsWzrnUPbbWAyggREQyzviZ\nhbG3HkABISKSUQrXbePvBfG3HkABISKSUTKl9QAKCBGRjJFJrQdQQIiIZIzy1kM6rhaXCgWEiEgG\nKG89jBrcnTZNG8RdDqCAEBHJCF+OPWRG6wEUECIiscvE1gMoIEREYpeJrQdQQIiIxCpTWw+ggBAR\nidU9Gdp6AAWEiEhslq7dxtQMbT2AAkJEJDbjZxXSJENbD6CAEBGJRaa3HkABISISi/LWw1UZ2nqA\nCAPCzCaZ2brw8qIVH7vBzNzM2oX3zczGm1mhmRWY2aCo6hIRidviNZnfeoBoWxCPAGdVXGhmXYFh\nwKcJi88GeoVfY4AHIqxLRCRWd728iGYN62Xs2EO5yALC3d8CNiZ56I/ATYAnLBsBPOaB94FWZtYp\nqtpEROLy3rINzFq0jrFDe9I6g1sPkOYxCDMbAax297kVHuoMrEq4XxQuS/YaY8ws38zyS0pKIqpU\nRKTmlZU5//XSQg5t2YjRg7vHXc5+pS0gzKwJ8Avgtuq8jrtPdPc8d89r3759zRQnIpIG0+YVU1C0\nhRuG9aZR/bpxl7Nf9dL4XkcAPYC5ZgbQBZhjZicAq4GuCet2CZeJiNQKu0v38ftXFnFUpxacPzBp\nB0nGSVsLwt3nuXsHd+/u7t0JupEGufsaYApwWXg004nAFncvTldtIiJRe+L9T1m1cRe3nN2HunUs\n7nJSEuVhrk8C7wG9zazIzK6sYvXpwHKgEPgzcE1UdYmIpNvWz/cyYdZSTu7ZjlN7tYu7nJRF1sXk\n7iP383j3hNsOjI2qFhGROD34xjI27dzLLWf3Iexizwo6k1pEJELFW3bx0DufcP6xh9Kvc8u4yzkg\nCggRkQjd/eoS3OGGYb3jLuWAKSBERCKyaM1WnptTxKjBh9G1TZO4yzlgCggRkYjc9VIwpcbYoT3j\nLuWgKCBERCLwj8L1vL64hLFDe9KqSWZPqVEZBYSISA0LptRYROdWjRmVBVNqVEYBISJSw6bOK2be\n6i3cMOzIrJhSozIKCBGRGrS7dB9/KJ9S49jsmFKjMgoIEZEa9NdwSo1bz+5DnSyZUqMyCggRkRqy\nZVcwpcYpvdpx6pHZP9u0AkJEpIY8+OYytuzay81n9Ym7lBqhgBARqQGfbd7FpHc+4bvHds66KTUq\no4AQEakBd88IptS4ftiRcZdSYxQQIiLVtLA4mFJj9JDudGmdfVNqVEYBISJSTXe9vIgWjeoz9rTs\nnFKjMgoIEZFqeLdwPW8sLuHaoT1p2aR+3OXUqCivKDfJzNaZ2fyEZX8ws0VmVmBmL5hZq4THbjWz\nQjNbbGZnRlWXiEhNCabUWEjnVo3595MOi7ucGhdlC+IR4KwKy2YA/dx9ALAEuBXAzPoCFwFHh8+5\n38yy9/x0EckJfy/4jPmrt/LzM7N7So3KRBYQ7v4WsLHCslfdvTS8+z7QJbw9AnjK3Xe7+ycE16Y+\nIaraRESqK5hSYzF9O7VgxDHZPaVGZeIcg7gCeCm83RlYlfBYUbhMRCQjPfLuCoo27eLW4dk/pUZl\nYgkIM/slUAo8cRDPHWNm+WaWX1JSUvPFiYjsR/GWXdwzcylnHNWBU3pl/5QalUl7QJjZaOBc4BJ3\n93DxaqBrwmpdwmVf4+4T3T3P3fPat6+9H4yIZK5x0xayr8y5/TtHx11KpNIaEGZ2FnATcJ6770x4\naApwkZk1NLMeQC/gn+msTUQkFe8WrmdqQTHXnNYzK68zfSDqRfXCZvYkcBrQzsyKgNsJjlpqCMww\nM4D33f1H7r7AzCYDHxN0PY11931R1SYicjD2lJZx+5QFdGvThKu/eXjc5UQusoBw95FJFj9Uxfrj\ngHFR1SMiUl0Pv/sJheu2M2l0Xq08rLUinUktIpKCLwemO/KtPh3jLictFBAiIin47RcD033jLiVt\nFBAiIvvxztL1TMuRgelECggRkSoEA9Pzc2ZgOlFkg9QiIrXBpHc/YVnJjpwZmE6kFoSISCU+27yL\n8Tk2MJ1IASEiUolxOTgwnUgBISKSxDtL1zNtXjFjh+bWwHQiBYSISAV7Ssu4bcp8DmvbhDGn5tbA\ndCINUouIVPDQO5+wvGQHD48+PucGphOpBSEikqB8YPrbfTsytE+HuMuJlQJCRCTBuGkLKXPntnNz\nc2A6kQJCRCT09tKSnB+YTqSAEBEhuMb07S8uyPmB6UQapBYRIRyYXr+Dhy/P7YHpRGpBiEjO+2zz\nLibMLGRY344M7Z3bA9OJIgsIM5tkZuvMbH7CsjZmNsPMlobfW4fLzczGm1mhmRWY2aCo6hIRqei3\n0z6mzJ1faWD6K6JsQTwCnFVh2S3ATHfvBcwM7wOcTXAd6l7AGOCBCOsSEfnCW0tKmD5vDddqYPpr\nIgsId38L2Fhh8Qjg0fD2o8D5Ccsf88D7QCsz6xRVbSIiEAxM3zFlAd3bNuGHGpj+mnSPQXR09+Lw\n9hqgfHrEzsCqhPWKwmUiIpH5y9vBwPTt5x2tgekkYhukdncH/ECfZ2ZjzCzfzPJLSkoiqExEcsHS\ntdu457WlnN3vEA1MVyLdAbG2vOso/L4uXL4a6JqwXpdw2de4+0R3z3P3vPbt20darIjUTqX7yvj5\nM3Np2rAuvx7RL+5yMla6A2IKMCq8PQp4MWH5ZeHRTCcCWxK6okREatTEt5czt2gLvx7Rj/bNG8Zd\nTsaK7EQ5M3sSOA1oZ2ZFwO3AncBkM7sSWAlcGK4+HRgOFAI7gcujqktEctuStdv404yga+ncAToW\npiqRBYS7j6zkodOTrOvA2KhqERGBoGvpxoSuJTOLu6SMpqk2RCRnlHctTRg5UF1LKdBUGyKSE9S1\ndOAUECJS65UftdSsUT1+c766llKVUkCY2U/NrEV4lNFDZjbHzIZFXZyISE34n7eWU1C0hV+POJp2\nzdS1lKpUWxBXuPtWYBjQGvh3giOSREQy2pLwhLjh/Q/h3AGHxl1OVkk1IMrbY8OBx919QcIyEZGM\nlNi1pBPiDlyqATHbzF4lCIhXzKw5UBZdWSIi1aeupepJ9TDXK4FjgeXuvtPM2qCT2UQkgy1eo66l\n6kq1BXESsNjdN5vZpcD/A7ZEV5aIyMEr3VfGjc+qa6m6Ug2IB4CdZnYMcAOwDHgssqpERKqhvGvp\nNyP6qWupGlINiNJwOowRwL3ufh/QPLqyREQOTnnX0jn9O3GOToirllTHILaZ2a0Eh7eeYmZ1gPrR\nlSUicuC+2rV0dNzlZL1UWxA/AHYTnA+xhuB6DX+IrCoRkYOQ2LXUVl1L1ZZSQISh8ATQ0szOBT53\nd41BiEjGWLxmG396bYm6lmpQqlNtXAj8E/g+wTUcPjCzC6IsTEQkVeVdSy0a1VfXUg1KdQzil8Dx\n7r4OwMzaA68Bz0ZVmIhIqsq7lu6/ZJC6lmpQqmMQdcrDIbThAJ4rIhKZL7qWBnRieH91LdWkVP/I\nv2xmr5jZaDMbDUwjuEzoQTGzn5nZAjObb2ZPmlkjM+thZh+YWaGZPW1mDQ729UUkN+wu3cf1kz8K\nupbOU9dSTUt1kPpGYCIwIPya6O43H8wbmlln4D+APHfvB9QFLgLuAv7o7j2BTQTTe4iIVOp30xay\n4LOt3PlvA9S1FIGULznq7s8Bz9Xg+zY2s71AE6AY+BZwcfj4o8AdBGdwi4h8zfR5xTz63kquOrkH\n3+7bMe5yaqUqA8LMtgGe7CHA3b3Fgb6hu682s/8PfArsAl4FZgOb3b00XK0I6FxJTWOAMQDdunU7\n0LcXkVpg5YYd3PxsAcd2bcVNZ/WJu5xaq8ouJndv7u4tknw1P5hwADCz1gRTdvQADgWaAmel+nx3\nn+juee6e1759+4MpQUSy2O7SfYz93znUqWPce/FAGtTT8TJRiWPLngF84u4l7r4XeB4YArQys/IW\nTRdgdQy1iUiG+920hcxfvZX//v4xdGndJO5yarU4AuJT4EQza2LBlcNPBz4GXgfKT74bBbwYQ20i\nksGmFQTjDj88pQdnaNwhcmkPCHf/gOAEuznAvLCGicDNwPVmVgi0BR5Kd20ikrlWrN/Bzc8VMLCb\nxh3SJeWjmGqSu98O3F5h8XLghBjKEZEM9/neYNyhbh1jwsiB1K+rcYd0iCUgREQOxO+mB+c7/OWy\nPI07pJFiWEQy2rSCYh7TuEMsFBAikrE07hAvBYSIZCSNO8RPYxAikpHGTdO4Q9wUySKScaYWfMbj\n769kzKmHa9whRgoIEckoK9bv4Jbn5jGwWytuPLN33OXkNAWEiGSMxHGHey8epHGHmGkMQkQyRuK4\nQ+dWjeMuJ+cpnkUkI2jcIfPkbEC4J7vMhYjE4ZNw3GGQxh0ySk4GxLyiLfzgf95n4449cZcikvN2\n7dnH2CfmUK+uMUHjDhklJz+JPfvK+KhoM2Mey+fzvfviLkckZ5WVOdc9/SEL12zljxceq3GHDJOT\nAXHcYa25+8JjyF+5iZueLVB3k0hM7nx5Ea8sWMuvzunL0D4d4i5HKsjZo5jOHXAoKzfs5A+vLKZ7\n2yZcP0z9niLp9MQHK5n41nIuO+kwLh/SPe5yJImcDQiAa047gpUbdjB+ViHd2jblguO6xF2SSE54\nc0kJt724gKG923PbuX0JLi4pmSaWLiYza2Vmz5rZIjNbaGYnmVkbM5thZkvD763TUAfjvtufwUe0\n5dbnC3hv2Yao31Ik5y1as5WxT8zhyI7NmXDxIOppUDpjxfXJ3AO87O59gGOAhcAtwEx37wXMDO9H\nrn7dOjxw6XEc1rYpP/rrbJaVbE/H24rkpHVbP+fKR/Jp2rAuk0bn0axhTndiZLy0B4SZtQROJbzm\ntLvvcffNwAjg0XC1R4Hz01VTy8b1eXj08dSrY1z+8L/YsH13ut5aJGfs3FPKVY/ls2nnHh4adTyd\nWuqIpUwXRwuiB1ACPGxmH5rZX8ysKdDR3YvDddYAaT2VsmubJvx5VB5rt37OmMdn6/BXkRq0r8y5\n7qmPmL96C+MvGki/zi3jLklSEEdA1AMGAQ+4+0BgBxW6kzw47jTpsadmNsbM8s0sv6SkpEYLG9St\nNXdfeCyzV27ixmcLKCvT4a8iNeHOlxby6sdr+dW5fTWNRhaJIyCKgCJ3/yC8/yxBYKw1s04A4fd1\nyZ7s7hPdPc/d89q3b1/jxZ0zoBM3n9WHv8/9jD++tqTGX18k1/z1/ZX8+e1PGD24O5cP6RF3OXIA\n0h4Q7r4GWGVm5ScenA58DEwBRoXLRgEvpru2cj/65uH8IK8rE2YV8kz+qrjKEMl6byxex+1TFvCt\nPh341bl94y5HDlBchxD8BHjCzBoAy4HLCcJqspldCawELoypNsyM3363H0Wbd/KLF+bRuXVjBh/R\nLq5yRLLSwuKtXPu/H9K7Y3MmjBxI3To61yHbWDZPM5GXl+f5+fmRvf6WXXu54IF/sHbr5zx/zRB6\ndmgW2XuJ1Cbrtn7O+fe9yz53/jZ2iI5YyjBmNtvd8/a3ns5QqULLxvWZNPp4GtSrwxWP6PBXkVTs\n3FPKlY/ms3nXXh3OmuUUEPvRtU0T/nxZcPjrDzX7q0iV9pU5P33qIxZ8toUJI3U4a7ZTQKRgYLfW\n/PEHxzLn0838/Jm5OvxVpBK/m76QGR+v5bZz+3L6UTqcNdspIFI0vH8nbjm7D1MLirl7hg5/Fano\n8fdW8NA7weGso3U4a62giVAOwNWnHs6K9Tu49/VCOrZoyL+f1D3ukkQywsvzi7l9ygJO1+GstYoC\n4gCYGb85vx/rt+/hVy8uAFBISM57aV4xP3nyQwZ2a814Hc5aq6iL6QDVr1uH+y8ZxBlHdeBXLy7g\n8fdWxF2SSGzKw+GYrq145PLjaarZWWsVBcRBaFCvDvdfcpxCQnJaxXBo3qh+3CVJDVNAHCSFhOQy\nhUNuUEBUQ8WQeOy9FXGXJBK5l+YVc63CIScoIKrpy5DoyG0KCanlysPh2K6tePSKExQOtZwCogYE\nITFIISG1WsVw0OVCaz8FRA1RSEhtpnDITQqIGqSQkNpI4ZC7FBA1TCEhtcl0hUNOU0BEoDwkvt1X\nISHZa3r5GdIKh5wVW0CYWV0z+9DMpob3e5jZB2ZWaGZPh1eby1oN6tXhvosVEpKdEsPhEYVDzoqz\nBfFTYGHC/buAP7p7T2ATcGUsVdUghYRkI4WDlIslIMysC3AO8JfwvgHfAp4NV3kUOD+O2mqaQkKy\nicJBEsXVgvgTcBNQFt5vC2x299LwfhHQOY7ColAxJCa+tYxsvha41E4vfFikcJCvSHtAmNm5wDp3\nn32Qzx9jZvlmll9SUlLD1UWnPCTO6d+J301fxC3PzWNPadn+nygSsbIy5/cvL+JnT8/l+O6tFQ7y\nhTj2giHAeWY2HGgEtADuAVqZWb2wFdEFWJ3sye4+EZgIkJeXl1X/hjeoV4cJIwdyePumTJhVyCcb\ndvDgpcfRpmlWj8dLFtuxu5SfPf0Rr368lpEndOU/z+tHg3o6uFECad8T3P1Wd+/i7t2Bi4BZ7n4J\n8DpwQbjaKODFdNeWDnXqGDcM6809Fx3LR6s2M+K+d1iydlvcZUkOWr15Fxc8+B6vLVzL7d/py+++\n21/hIF+RSXvDzcD1ZlZIMCbxUMz1RGrEsZ2ZfPVJfL63jO/d/w9mLVobd0mSQ2av3MSIe9+haONO\nJo0+nsuH9CA4VkTkS5bNg6V5eXmen58fdxnVUrxlFz98LJ8Fn23lF2cfxVWn6BdVovX8nCJueW4e\nnVo14qFRefTs0DzukiTNzGy2u+ftb71MakHkpE4tGzP56pM4u98hjJu+kJueLdDgtUSirMy56+VF\nXD95LoMOa8XfrhmicJAq6VCFDNCkQT3uHTmIP7VfwvhZhazcsJMHLh1E22YN4y5Naokdu0u57umP\nmPHxWi7+Rjf+87yjqV9X/x9K1bSHZIg6dYzrh/Vm/MiBzC3azIj73mXxGg1eS/UVbdrJvz3wD2Yu\nXMsd3+nLuPP7KRwkJdpLMsx5xxzK01efxJ7SMr53/7vMXKjBazl4s1du5Pz73mX15l08cvkJjNZg\ntBwABUQGOrZrK6ZcezI92jflqsfydea1HJTnZhcxcuIHNGtYjxeuGcKpR7aPuyTJMgqIDHVIy0Y8\nc/VghvcLzry+6dkCdpfui7ssyQL7ypw7X1rEDc/MJa97a/42dgg9OzSLuyzJQhqkzmCNG9RlwsiB\n9OzQjHtmLmVFeOa1Bq+lMtt3l3LdUx/x2sK1XPKNbtyhwWipBu05Ga5OHeNn3z6SCSMHUlC0hXPG\nv8Pri9fFXZZkoNkrN3LehGD/+M/zjua3GoyWatLekyW+c8yhPPfjwTRvVI/LH/4XNz4zly279sZd\nlmSAz/fuY9y0j7ngwffYXVrG41eewKjB3TUYLdWmLqYs0q9zS6b+x8mMn7mUB99czttL1/Nf3+vP\n0D4d4i5NYjJ75UZufKaA5et3cMk3unHr8KM0E6vUGLUgskzDenW58cw+vHDNYFo0rsflj/yLn6s1\nkXM+37uP3079stXwxFXfYNx3+yscpEZpb8pSA7q04u8/+bI18Y5aEzlDrQZJF7UgsphaE7lFrQZJ\nN+1ZtUDF1sTbS0u483sD1JqoRfJXbOSmZ9VqkPRSC6KWSGxNtGxcX62JWmLXnqDV8P3/UatB0k97\nWS1T3pqYMLOQB95cptZEFstfsZEbny3gk/U7uPTEbtxytloNkl5qQdRCDevV5edn9uaFawbTqnED\ntSayTGKrYU9pGf971Tf47flqNUj6pX2PM7OuwGNAR8CBie5+j5m1AZ4GugMrgAvdfVO666tNBnRp\nxZSfDPmiNfHmkhKuOe0IRp7QjUb168ZdnlRQuq+MFz5czfhZS1m1cZdaDRK7tF9y1Mw6AZ3cfY6Z\nNQdmA+cDo4GN7n6nmd0CtHb3m6t6rdpwydF0mVe0hXHTP+b95Rvp0LwhP1ZQZIy9YTDcO6uQTzfu\npF/nFvxi+FEMPqJd3KVJLZXqJUdjvya1mb0I3Bt+nebuxWGIvOHuvat6rgLiwL23bAP3zFyioMgA\nyYLhutOP5PSjOmiaDIlUVgSEmXUH3gL6AZ+6e6twuQGbyu9XeM4YYAxAt27djlu5cmXa6q1NEoOi\nffOG/PibR3DxNxQU6VAxGPp3bsl1Z/TiW30UDJIeGR8QZtYMeBMY5+7Pm9nmxEAws03u3rqq11AL\novoUFOmzd18ZL8xZzYTXgzEGBYPEJaMDwszqA1OBV9z97nDZYtTFFJv3l2/gnteW8t7yDQqKGqZg\nkEyTsQERdh89SjAgfV3C8j8AGxIGqdu4+01VvZYCouZVDIofffMILlFQHBQFg2SqTA6Ik4G3gXlA\nWbj4F8AHwGSgG7CS4DDXjVW9lgIiOhWD4upTD+ffBnWhddMGcZeW8bZ9vpepBcXc/0YhqzbuYkCX\nIBiG9lYwSGbI2ICoSQqI6CUGRd06xuAj2nJO/06cefQhCosE2z7fy8yF65g2r5g3l5Swp7RMwSAZ\nSwEhNWr+6i1Mm1fM9HnFrNywU2FB8lA4pEUjzu5/COf078Rxh7VWMEhGUkBIJNydBZ9trTQshh19\nCG1qcViUh8LUgmLeWvrVUDh3QCcGdm1NnToKBclsCgiJXHlYTJ9XzLRaHBaVhcLw/p04Z8AhCgXJ\nOgoISauqwmJ4/04c3701h7VtSv26mT8/5L4yZ9XGnXy4ahPTCtYoFKTWUUBIbJKFBUD9usbh7ZrR\ns2MzjuzQnCM7NqNXx2axBUd5ECxZu42l67azdO02lqzdzrKS7ewuDQ6wUyhIbaSAkIzg7ixZu50F\nn21hydrtFK4L/giv2rST8l0v6uBIJQgADm3ZiF4dw/fv0JyjOrXg6ENbKBSk1kk1IDSPsETKzOh9\nSHN6H9L8K8t37dnHspLtLAn/WBeu28a8oi1Mn1f8teBo07QBB3sw0Oade78WBJ1bNaZnh2YM6dmW\nXh2b06tDM3p2aEbzRvUP9scUqZUUEBKLxg3q0q9zS/p1bvmV5cmCozoXOurQouFXgqBXx+a6voJI\nivSbIhmlsuAQkfTL/ENKREQkFgoIERFJSgEhIiJJKSBERCQpBYSIiCSlgBARkaQUECIikpQCQkRE\nksrquZjMrITg8qQHox2wvgbLqSmZWhdkbm2q68CorgNTG+s6zN3b72+lrA6I6jCz/FQmq0q3TK0L\nMrc21XVgVNeByeW61MUkIiJJKSBERCSpXA6IiXEXUIlMrQsytzbVdWBU14HJ2bpydgxCRESqlsst\nCBERqUKtDggz+76ZLTCzMjPLq/DYrWZWaGaLzezMSp7fw8w+CNd72swaRFDj02b2Ufi1wsw+qmS9\nFWY2L1wv8uusmtkdZrY6obbhlax3VrgNC83sljTU9QczW2RmBWb2gpm1qmS9tGyv/f38ZtYw/IwL\nw32pe1S1JLxnVzN73cw+Dvf/nyZZ5zQz25Lw+d4WdV0J713lZ2OB8eE2KzCzQWmoqXfCtvjIzLaa\n2XUV1knLNjOzSWa2zszmJyxrY2YzzGxp+L11Jc8dFa6z1MxGVbsYd6+1X8BRQG/gDSAvYXlfYC7Q\nEOgBLAPqJnn+ZOCi8PaDwI8jrve/gdsqeWwF0C6N2+4O4Of7WaduuO0OBxqE27RvxHUNA+qFt+8C\n7opre6Xy8wPXAA+Gty8Cnk7DZ9cJGBTebg4sSVLXacDUdO1PB/LZAMOBlwADTgQ+SHN9dYE1BOcK\npH2bAacCg4D5Cct+D9wS3r4l2X4PtAGWh99bh7dbV6eWWt2CcPeF7r44yUMjgKfcfbe7fwIUAick\nrmBmBnwLeDZc9ChwflS1hu93IfBkVO8RgROAQndf7u57gKcItm1k3P1Vdy8N774PdIny/fYjlZ9/\nBMG+A8G+dHr4WUfG3YvdfU54exuwEOgc5XvWsBHAYx54H2hlZp3S+P6nA8vc/WBPwq0Wd38L2Fhh\nceJ+VNkvox4ZAAAFS0lEQVTfojOBGe6+0d03ATOAs6pTS60OiCp0BlYl3C/i679AbYHNCX+Mkq1T\nk04B1rr70koed+BVM5ttZmMirCPRtWETf1IlTdpUtmOUriD4TzOZdGyvVH7+L9YJ96UtBPtWWoRd\nWgOBD5I8fJKZzTWzl8zs6HTVxP4/m7j3q4uo/B+1uLZZR3cvDm+vATomWafGt1vWX5PazF4DDkny\n0C/d/cV015NMijWOpOrWw8nuvtrMOgAzzGxR+J9GJHUBDwC/Ifhl/g1B99cV1Xm/mqirfHuZ2S+B\nUuCJSl6mxrdXtjGzZsBzwHXuvrXCw3MIulC2h+NLfwN6pam0jP1swnHG84Bbkzwc5zb7gru7maXl\n8NOsDwh3P+MgnrYa6Jpwv0u4LNEGgqZtvfA/v2Tr1EiNZlYP+B5wXBWvsTr8vs7MXiDo3qjWL1Wq\n287M/gxMTfJQKtuxxusys9HAucDpHna+JnmNGt9eSaTy85evUxR+zi0J9q1ImVl9gnB4wt2fr/h4\nYmC4+3Qzu9/M2rl75HMOpfDZRLJfpehsYI67r634QJzbDFhrZp3cvTjsbluXZJ3VBOMk5boQjL8e\ntFztYpoCXBQeYdKD4L+AfyauEP7heR24IFw0CoiqRXIGsMjdi5I9aGZNzax5+W2Cgdr5ydatKRX6\nfL9byfv9C+hlwdFeDQia5lMiruss4CbgPHffWck66dpeqfz8Uwj2HQj2pVmVhVpNCcc4HgIWuvvd\nlaxzSPlYiJmdQPC3IB3BlcpnMwW4LDya6URgS0L3StQqbcnHtc1CiftRZX+LXgGGmVnrsEt4WLjs\n4EU9Ih/nF8EftiJgN7AWeCXhsV8SHIGyGDg7Yfl04NDw9uEEwVEIPAM0jKjOR4AfVVh2KDA9oY65\n4dcCgq6WqLfd48A8oCDcOTtVrCu8P5zgKJllaaqrkKCf9aPw68GKdaVzeyX7+YFfEwQYQKNw3ykM\n96XD07CNTiboGixI2E7DgR+V72fAteG2mUsw2D846rqq+mwq1GbAfeE2nUfCEYgR19aU4A9+y4Rl\nad9mBAFVDOwN/35dSTBuNRNYCrwGtAnXzQP+kvDcK8J9rRC4vLq16ExqERFJKle7mEREZD8UECIi\nkpQCQkREklJAiIhIUgoIERFJSgEhIiJJKSBEEphZKzO7Jrx9mpklO4O8quePNrNDD/K9U5rSWSRd\nFBAiX9WKYIrugzWa4KS9g3ELMNPdexGcFBX59TVEqqIT5UQSmFn5lN2LCc5k3QGsB/oBs4FL3d3N\n7DjgbqBZ+PhoYAjBWfGrgV3AScCNwHeAxsA/gKu9kl86M1sMnOZfzrfzhrv3juYnFdk/BYRIgnB6\n7Knu3s/MTiOY8+Zo4DPgXYI/+B8AbwIj3L3EzH4AnOnuV5jZGwQXWsoPX6+Nu28Mbz8OTHb3v1fy\n3pvdvVV424BN5fdF4pD1s7mKROyfHk6iaMHlYLsDmwlaFDPCudvqEsydk8xQM7sJaEJwpa8FQNKA\nSBS2UvTfm8RKASFStd0Jt/cR/M4YsMDdT6rqiWbWCLifYLK5VWZ2B8HkfZVJZUpnkbTRILXIV20j\nuI5zVRYD7c3sJAiuvZBwdbHE55eHwfrw4j0XULVUpnQWSRu1IEQSuPsGM3vXzOYTDDQnu3DMHjO7\nABhvZi0Jfo/+RNB99AjwoJmVD1L/meB6B2sIrh9RlTuByWZ2JbCS4BrlIrHRILWIiCSlLiYREUlK\nXUwiaWZm9xGcM5HoHnd/OI56RCqjLiYREUlKXUwiIpKUAkJERJJSQIiISFIKCBERSUoBISIiSf0f\nt6vSuqyDmngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2d131d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "theta_0 = np.arange(21)-10\n",
    "theta_1 = np.array([0.5,0.8]) # One parameter for each feature\n",
    "\n",
    "loss = []\n",
    "for t_0 in theta_0:\n",
    "    loss.append(loss_function(y, hypothesis_function(x, t_0, theta_1)))\n",
    "    \n",
    "plt.plot(theta_0, loss)\n",
    "plt.xlabel('theta_0')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `theta_0` that minimises the loss function can then be chosen. However this assumed that the values in `theta_1` are fixed, and so this value is unlikely to be a global minimum. An algorithm is required to minimise the loss function over all of the parameter values at the same time to find a global minimum. This is the fundamental concept of machine learning algorithms.\n",
    "\n",
    "There are numerous minimisation techniques that can be employed to find the optimum set of parameter values, but the most common is *gradient descent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "This is an algorithm that adjusts the values of the parameters slowly to find a minimum for the loss function. It works by gradually adjusting the values of the parameters in the direction of maximum negative gradient; the direction that minimises the loss function in the local region. After taking very many small steps in the calculated direction, the algorithm will converge to a steady-state minimum loss value. The update rule is as follows:\n",
    "\n",
    "theta_i := theta_i - alpha * grad(loss_function w.r.t. theta_i)\n",
    "\n",
    "Here alpha is the *learning rate*. This is a *hyper-parameter* of the algorithm and needs to be set according to the problem being solved. If the learning rate is too large, then it can cause the algorithm to explode to infinity, but if the value is too small then the rate of learning can become very slow.\n",
    "\n",
    "The second term is the gradient of the loss function with respect to theta_i\n",
    "\n",
    "This gradient can be calculated based on the loss function being used. For the case of linear regression, the gradients are reasonably easy to calculate, and are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, y, theta_0, theta_1, learning_rate=0.01):\n",
    "    new_theta_0 = theta_0 - learning_rate * (np.sum(hypothesis_function(x, theta_0, theta_1) - y, axis=0) / len(y))\n",
    "    new_theta_1 = theta_1 - learning_rate * (np.matmul((hypothesis_function(x, theta_0, theta_1) - y), x) / len(y))\n",
    "    return new_theta_0, new_theta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this definition of the gradient we have everything we need to run the linear regression algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0:  -1.86334771099\n",
      "theta_1:  [ 3.28128319 -0.06500692]\n",
      "Final Loss:  0.341294096642\n"
     ]
    }
   ],
   "source": [
    "# Initialise parameter values\n",
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1])\n",
    "\n",
    "# Gradually update the parameter values\n",
    "n = 0\n",
    "while n < 20000:\n",
    "    theta_0, theta_1 = gradient_descent_update(x, y, theta_0, theta_1)\n",
    "    n += 1\n",
    "    \n",
    "print('theta_0: ', theta_0)\n",
    "print('theta_1: ', theta_1)\n",
    "print('Final Loss: ', loss_function(y, hypothesis_function(x, theta_0, theta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn\n",
    "\n",
    "Sci-kit learn is a python library which has a large number of inbuilt machine learning algorithm, including linear regression. This is run in a handful of lines of code, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0:  -1.86339092873\n",
      "theta_1:  [ 3.28131749 -0.0650108 ]\n",
      "Loss:  0.341294096472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(x, y)\n",
    "\n",
    "print('theta_0: ', lm.intercept_)\n",
    "print('theta_1: ', lm.coef_)\n",
    "\n",
    "loss = loss_function(y, hypothesis_function(x, lm.intercept_, lm.coef_))\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the output matches the output of the method we implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tips and Tricks\n",
    "\n",
    "### Parameter Scaling\n",
    "When performing gradient descent then it is important to ensure that all of our features are on a similar scale. This is because if one feature is orders of magnitude greater than another then the gradients become skewed, causing the algorithm  to either take much longer or fail to converge entirely.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0:  5.78520430494\n",
      "theta_1:  [ 114.51793105   -1.41469502]\n",
      "Final Loss:  267297952.518\n"
     ]
    }
   ],
   "source": [
    "# Data - feature 1 is the size of the house in squared feet and feature 2 is the number of bedrooms\n",
    "# Data - y is the value o the house\n",
    "x = np.array([[2000,3],[1800,2],[1200,1],[1350,2],[1670,2],[1942,3]])\n",
    "y = np.array([230000, 190000, 156000, 180000, 195000, 204000])\n",
    "\n",
    "# Initialise parameter values\n",
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1])\n",
    "\n",
    "# Gradually update the parameter values\n",
    "n = 0\n",
    "while n < 20000:\n",
    "    theta_0, theta_1 = gradient_descent_update(x, y, theta_0, theta_1, learning_rate=0.0000001)\n",
    "    n += 1\n",
    "    \n",
    "print('theta_0: ', theta_0)\n",
    "print('theta_1: ', theta_1)\n",
    "print('Final Loss: ', loss_function(y, hypothesis_function(x, theta_0, theta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the algorithm fails to converge unless the learning rate is reduced significantly, and then it doesn't converge within the 20000 iteration limit. However, if we simply scale the feature vectors to be of the same order of magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0:  94773.1703883\n",
      "theta_1:  [ 38366.32717302  15685.27204531]\n",
      "Final Loss:  56247568.1267\n"
     ]
    }
   ],
   "source": [
    "# Data - feature 1 is the size of the house in squared feet and feature 2 is the number of bedrooms\n",
    "# Data - y is the value o the house\n",
    "x = np.array([[2000.,3],[1800.,2],[1200.,1],[1350.,2],[1670.,2],[1942.,3]])\n",
    "y = np.array([230000, 190000, 156000, 180000, 195000, 204000])\n",
    "\n",
    "# Scale the first feature\n",
    "x[:,0] = np.divide(x[:,0], 1000.0)\n",
    "\n",
    "# Initialise parameter values\n",
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1])\n",
    "\n",
    "# Gradually update the parameter values\n",
    "n = 0\n",
    "while n < 20000:\n",
    "    theta_0, theta_1 = gradient_descent_update(x, y, theta_0, theta_1)\n",
    "    n += 1\n",
    "    \n",
    "print('theta_0: ', theta_0)\n",
    "print('theta_1: ', theta_1)\n",
    "print('Final Loss: ', loss_function(y, hypothesis_function(x, theta_0, theta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scaled feature set, the solution converges appropriately using the standard learning rate within the 20000 iterations.\n",
    "\n",
    "### Mean Normalization\n",
    "Scaling can also be achieved by 'mean normalization' where the features are scaled around the mean and standard deviation of the feature space, forming a normal distribution about the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15100102  1.21267813]\n",
      " [ 0.47327716 -0.24253563]\n",
      " [-1.55989442 -1.69774938]\n",
      " [-1.05160152 -0.24253563]\n",
      " [ 0.03275665 -0.24253563]\n",
      " [ 0.9544611   1.21267813]]\n",
      "theta_0:  192500.0\n",
      "theta_1:  [  9942.98516676  11897.68554356]\n",
      "Final Loss:  55835546.0362\n"
     ]
    }
   ],
   "source": [
    "# Data - feature 1 is the size of the house in squared feet and feature 2 is the number of bedrooms\n",
    "# Data - y is the value o the house\n",
    "x = np.array([[2000.,3],[1800.,2],[1200.,1],[1350.,2],[1670.,2],[1942.,3]])\n",
    "y = np.array([230000, 190000, 156000, 180000, 195000, 204000])\n",
    "\n",
    "# Scale the first feature\n",
    "x[:,0] = np.divide(x[:,0]-np.mean(x[:,0]), np.std(x[:,0]))\n",
    "x[:,1] = np.divide(x[:,1]-np.mean(x[:,1]), np.std(x[:,1]))\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Initialise parameter values\n",
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1])\n",
    "\n",
    "# Gradually update the parameter values\n",
    "n = 0\n",
    "while n < 20000:\n",
    "    theta_0, theta_1 = gradient_descent_update(x, y, theta_0, theta_1)\n",
    "    n += 1\n",
    "    \n",
    "print('theta_0: ', theta_0)\n",
    "print('theta_1: ', theta_1)\n",
    "print('Final Loss: ', loss_function(y, hypothesis_function(x, theta_0, theta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Min Scaling\n",
    "An alternative method is the 'max-min' scaling method which puts all values in the range 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.      0.    ]\n",
      " [ 0.25    0.5   ]\n",
      " [ 1.      1.    ]\n",
      " [ 0.8125  0.5   ]\n",
      " [ 0.4125  0.5   ]\n",
      " [ 0.0725  0.    ]]\n",
      "theta_0:  218359.606711\n",
      "theta_1:  [-27417.23622353 -34125.32472196]\n",
      "Final Loss:  55842088.3469\n"
     ]
    }
   ],
   "source": [
    "# Data - feature 1 is the size of the house in squared feet and feature 2 is the number of bedrooms\n",
    "# Data - y is the value o the house\n",
    "x = np.array([[2000.,3],[1800.,2],[1200.,1],[1350.,2],[1670.,2],[1942.,3]])\n",
    "y = np.array([230000, 190000, 156000, 180000, 195000, 204000])\n",
    "\n",
    "# Scale the first feature\n",
    "x[:,0] = np.divide(np.max(x[:,0])-x[:,0], np.max(x[:,0])-np.min(x[:,0]))\n",
    "x[:,1] = np.divide(np.max(x[:,1])-x[:,1], np.max(x[:,1])-np.min(x[:,1]))\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Initialise parameter values\n",
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1])\n",
    "\n",
    "# Gradually update the parameter values\n",
    "n = 0\n",
    "while n < 20000:\n",
    "    theta_0, theta_1 = gradient_descent_update(x, y, theta_0, theta_1)\n",
    "    n += 1\n",
    "    \n",
    "print('theta_0: ', theta_0)\n",
    "print('theta_1: ', theta_1)\n",
    "print('Final Loss: ', loss_function(y, hypothesis_function(x, theta_0, theta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sci-kit learn has some inbuilt functions that perform data normalization for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48417828  0.53881591]\n",
      " [ 0.43576045  0.3592106 ]\n",
      " [ 0.29050697  0.1796053 ]\n",
      " [ 0.32682034  0.3592106 ]\n",
      " [ 0.40428886  0.3592106 ]\n",
      " [ 0.47013711  0.53881591]]\n",
      "theta_0:  109222.046315\n",
      "theta_1:  [  94210.33704912  116780.04647577]\n",
      "Final Loss:  58116675.4388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Data - feature 1 is the size of the house in squared feet and feature 2 is the number of bedrooms\n",
    "# Data - y is the value o the house\n",
    "x = np.array([[2000.,3],[1800.,2],[1200.,1],[1350.,2],[1670.,2],[1942.,3]])\n",
    "y = np.array([230000, 190000, 156000, 180000, 195000, 204000])\n",
    "\n",
    "x = normalize(x, axis=0)\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Initialise parameter values\n",
    "theta_0 = 1\n",
    "theta_1 = np.array([1,1])\n",
    "\n",
    "# Gradually update the parameter values\n",
    "n = 0\n",
    "while n < 20000:\n",
    "    theta_0, theta_1 = gradient_descent_update(x, y, theta_0, theta_1)\n",
    "    n += 1\n",
    "    \n",
    "print('theta_0: ', theta_0)\n",
    "print('theta_1: ', theta_1)\n",
    "print('Final Loss: ', loss_function(y, hypothesis_function(x, theta_0, theta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly each of the proposed methods gives different results, each with very similar final loss values. This is likely because our dataset is very small and so there are a large number of similarly effective solutions (local minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_Coursera]",
   "language": "python",
   "name": "conda-env-ML_Coursera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

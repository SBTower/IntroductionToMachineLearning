{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are a particularly technique that has been shown to be very useful in machine learning applications, in particular in classification.\n",
    "\n",
    "We have previously explored the application of simple machine learning techniques that are well equipped for learning functions that are able to classify simple problems, but do not scale well to highly non-linear problems with a large number of features. For example, in the problem of computer vision there are a large number of features taken from the pixels in the image. Using a classic logistic regression technique will require a very large number of features, as well as combinations of features, makes it expensive to train. Neural Networks provide a method to counter these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "Neural networks have been developed to simulate the neurons in the human brain. Multiple neurons are arranged in a network, and 'fire' at different rates to send signals around the brain and out into the body. Each neuron takes a signal as an input, performs some function to adjust that input signal and fires out the adjusted output signal to the next neuron in the network.\n",
    "\n",
    "The neurons in our Artificial Neural Network (ANN) mimic this behaviour. Each neuron takes multiple values as input, and then combines and adjusts the values by applying a bias and a weight. The output signal is then *activated* through an activation function before being input to the neurons in the next layer of the network. This is shown in the diagram below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/neuron_model_logistic_unit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common neuron model is the *logistic unit*, which uses the sigmoid function as its activation function. As a reminder, the sigmoid function is a shown\n",
    "\n",
    "`1 / (1+exp(z))`\n",
    "\n",
    "where z is equal to `theta * x`, with x being the input features and theta the parameters (or weights) to be learnt as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a neural network multiple of these units are connected in parallel as shown in the diagram below. The input layer is made up of the features as in the previous machine learning techniques. These are fed into multiple units forming the first *hidden layer* of the network. Each unit performs its own activation on the inputs, and then passes its output into the next layer of the network. The network will have a number of hidden layers, before the final *output layer* outputs the final value(s) from the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture has been proven to be able to approximate *any* function, so long as the weights and biases in each unit are set to the appropriate values. Therefore these can be used to learn the complex functions that map input features to the output labels if we can find a way to train the network appropriately. This is achieved through a process known as *back-propagation* which we will get into later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value output by each unit is controlled by the weights parameterised within it. Each input is multiplied by the corresponding *weight*, and then each of these combinations is summed together. This final value is then *activated* by the activation function (often the sigmoid function).\n",
    "\n",
    "`output = a(x1*w1 + x2*w2 + x3*w3 + ... + xn*wn + b)` where `a(z) = 1/(1+exp(z))` for a logistic unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of mapping the inputs through the network to generate the output is known as *forward propagation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_Coursera]",
   "language": "python",
   "name": "conda-env-ML_Coursera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
